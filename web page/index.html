<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Skribix - Handdrawn Sketch Recognition</title>
    <style>
        :root {
            --white: #ffffff;
            --light-gray: #f5f5f5;
            --red: #e63946;
            --dark-red: #c1121f;
            --black: #1d3557;
            --light-black: #333333;
        }
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        }
        
        body {
            background-color: var(--white);
            color: var(--black);
            line-height: 1.6;
        }
        
        header {
            background-color: var(--black);
            color: var(--white);
            padding: 2rem 0;
            text-align: center;
            box-shadow: 0 2px 5px rgba(0,0,0,0.2);
            position: relative;
        }
        
        .logo {
            font-size: 3rem;
            font-weight: bold;
            color: var(--red);
            margin-bottom: 0.5rem;
        }
        
        .subtitle {
            font-size: 1.5rem;
            opacity: 0.9;
        }
        
        /* Navigation Icon */
        .nav-toggle {
            position: fixed;
            top: 1rem;
            right: 1rem;
            z-index: 1001;
            background-color: var(--red);
            color: white;
            border: none;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            cursor: pointer;
            box-shadow: 0 2px 10px rgba(0,0,0,0.3);
            transition: background-color 0.3s;
        }
        
        .nav-toggle:hover {
            background-color: var(--dark-red);
        }
        
        .nav-icon {
            position: relative;
            width: 24px;
            height: 18px;
        }
        
        .nav-icon span {
            position: absolute;
            height: 3px;
            width: 100%;
            background-color: white;
            border-radius: 3px;
            transition: all 0.3s ease;
        }
        
        .nav-icon span:nth-child(1) {
            top: 0;
        }
        
        .nav-icon span:nth-child(2) {
            top: 7px;
        }
        
        .nav-icon span:nth-child(3) {
            top: 14px;
        }
        
        /* Active state for hamburger to X transition */
        .nav-toggle.active .nav-icon span:nth-child(1) {
            transform: rotate(45deg);
            top: 7px;
        }
        
        .nav-toggle.active .nav-icon span:nth-child(2) {
            opacity: 0;
        }
        
        .nav-toggle.active .nav-icon span:nth-child(3) {
            transform: rotate(-45deg);
            top: 7px;
        }
        
        /* Sidebar Navigation */
        .sidebar {
            position: fixed;
            top: 0;
            right: -300px;
            width: 300px;
            height: 100%;
            background-color: var(--black);
            padding-top: 80px;
            transition: right 0.3s ease;
            z-index: 1000;
            box-shadow: -2px 0 10px rgba(0,0,0,0.2);
            overflow-y: auto;
        }
        
        .sidebar.active {
            right: 0;
        }
        
        .sidebar ul {
            list-style-type: none;
        }
        
        .sidebar ul li {
            margin-bottom: 0.5rem;
        }
        
        .sidebar ul li a {
            color: var(--white);
            text-decoration: none;
            font-weight: 500;
            padding: 1rem 2rem;
            display: block;
            transition: background-color 0.3s;
            border-left: 4px solid transparent;
        }
        
        .sidebar ul li a:hover {
            background-color: var(--light-black);
            border-left: 4px solid var(--red);
        }
        
        /* Overlay for when sidebar is active */
        .overlay {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background-color: rgba(0,0,0,0.5);
            z-index: 999;
            display: none;
            opacity: 0;
            transition: opacity 0.3s ease;
        }
        
        .overlay.active {
            display: block;
            opacity: 1;
        }
        
        .container {
            max-width: 1200px;
            margin: 2rem auto;
            padding: 0 2rem;
        }
        
        section {
            margin-bottom: 3rem;
            padding: 2rem;
            background-color: var(--white);
            border-radius: 8px;
            box-shadow: 0 2px 15px rgba(0,0,0,0.1);
        }
        
        h2 {
            color: var(--red);
            font-size: 2rem;
            margin-bottom: 1.5rem;
            padding-bottom: 0.5rem;
            border-bottom: 2px solid var(--light-gray);
        }
        
        h3 {
            color: var(--dark-red);
            font-size: 1.5rem;
            margin: 1.5rem 0 1rem;
        }
        
        p {
            margin-bottom: 1rem;
        }
        
        .process-list {
            margin-left: 2rem;
            margin-bottom: 1.5rem;
        }
        
        .process-list li {
            margin-bottom: 0.5rem;
        }
        
        .stats {
            background-color: var(--light-gray);
            padding: 1.5rem;
            border-radius: 6px;
            margin: 1.5rem 0;
        }
        
        .stats ul {
            list-style-type: none;
        }
        
        .stats li {
            padding: 0.5rem 0;
            border-bottom: 1px solid #ddd;
        }
        
        .stats li:last-child {
            border-bottom: none;
        }
        
        .feature-box {
            background-color: var(--light-gray);
            padding: 1.5rem;
            border-radius: 6px;
            margin: 1rem 0;
            border-left: 4px solid var(--red);
        }
        
        .highlight {
            color: var(--red);
            font-weight: 600;
        }
        
        .version-card {
            background-color: var(--white);
            border: 1px solid var(--light-gray);
            padding: 1.5rem;
            border-radius: 8px;
            margin-bottom: 1.5rem;
            box-shadow: 0 2px 10px rgba(0,0,0,0.05);
        }
        
        .version-card h3 {
            margin-top: 0;
            display: flex;
            align-items: center;
        }
        
        .version-number {
            background-color: var(--red);
            color: white;
            padding: 0.2rem 0.6rem;
            border-radius: 4px;
            margin-right: 0.5rem;
            font-size: 1rem;
        }
        
        footer {
            background-color: var(--black);
            color: var(--white);
            text-align: center;
            padding: 2rem 0;
            margin-top: 4rem;
        }
        
        .footer-text {
            opacity: 0.8;
            font-size: 0.9rem;
            margin-top: 1rem;
        }

        /* Scroll-to-top button */
        .scroll-top {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            background-color: var(--red);
            color: white;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            cursor: pointer;
            opacity: 0;
            visibility: hidden;
            transition: opacity 0.3s, visibility 0.3s;
            box-shadow: 0 2px 10px rgba(0,0,0,0.3);
            z-index: 998;
        }
        
        .scroll-top.active {
            opacity: 1;
            visibility: visible;
        }
        
        .scroll-top:hover {
            background-color: var(--dark-red);
        }
    </style>
</head>
<body>
    <header>
        <div class="logo">
            <img src="skribixlogo.png" alt="Skribix Logo" style="width: 300px; height: auto;">
        </div>
        <div class="subtitle">Handdrawn Sketch Recognition</div>
    </header>
    
    <!-- Navigation Toggle Button -->
    <button class="nav-toggle" id="navToggle">
        <div class="nav-icon">
            <span></span>
            <span></span>
            <span></span>
        </div>
    </button>
    
    <!-- Sidebar Navigation -->
    <div class="sidebar" id="sidebar">
        <ul>
            <li><a href="#introduction">Introduction</a></li>
            <li><a href="#dataset">Dataset Collection</a></li>
            <li><a href="#features">Feature Extraction</a></li>
            <li><a href="#methodologies">Methodologies</a></li>
            <li><a href="#versions">Project Versions</a></li>
        </ul>
    </div>
    
    <!-- Overlay -->
    <div class="overlay" id="overlay"></div>
    
    <div class="container">
        <section id="introduction">
            <h2>Introduction</h2>
            <p>Skribix is a project focused on handdrawn sketch recognition. The dataset used in this project includes sketches of objects from 250 distinct categories, selected based on day-to-day functionality criteria.</p>
            <img src="header_image.jpg" alt="Header Image">
        </section>
        
        <section id="dataset">
            <h2>Dataset Collection</h2>
            <p>The dataset is obtained from "How Do Humans Sketch Objects?". This dataset includes 20,000 sketches collected using crowd-sourcing platforms like Amazon Mechanical Turk (AMT).</p>
            
            <h3>Collection Process</h3>
            <ol class="process-list">
                <li><strong>Task Instructions:</strong> Participants were asked to sketch objects based on randomly assigned category names within a 30-minute time limit. They were instructed to draw outlines only, avoiding excessive context. Participants were not professionals.</li>
                <li><strong>Tools Provided:</strong> Participants defined their sketches using a stroke-based sketching canvas with undo, redo, clear, and delete options.</li>
                <li><strong>Quality Control:</strong> After collection, sketches were manually reviewed to remove incorrect or offensive entries while retaining poorly drawn but valid sketches.</li>
            </ol>
            
            <div class="stats">
                <h3>Crowd-Sourcing Details</h3>
                <ul>
                    <li><strong>Participants:</strong> 1,350 unique workers contributed sketches.</li>
                    <li><strong>Time Investment:</strong> A total of 741 hours was spent drawing sketches.</li>
                    <li><strong>Median drawing time per sketch:</strong> 86 seconds.</li>
                    <li><strong>Median number of strokes per sketch:</strong> 13.</li>
                    <li><strong>Drawing Pattern:</strong> Longer strokes were observed at the beginning of sketches, indicating a coarse-to-fine drawing strategy.</li>
                </ul>
            </div>
            
            <h3>Data Verification</h3>
            <p>To ensure quality:</p>
            <ul class="process-list">
                <li>Sketches were visually inspected using an interactive tool developed by the same university.</li>
                <li>About 6.3% of sketches were removed for being in the wrong category or not meeting requirements.</li>
                <li>For uniformity, the dataset was truncated to contain exactly 80 sketches per category.</li>
            </ul>
            
            <h3>Project Datasets</h3>
            <p>We created two sets of datasets depending on our version of the project:</p>
            <ul class="process-list">
                <li>The first implementation includes all 250 categories, consisting of 20,000 images.</li>
                <li>In the second implementation, we took a subset of 15 classes: Airplane, Book, Cup, Envelope, Fan, Fork, Hat, Key, Laptop, Leaf, Moon, Pizza, T-shirt, Traffic light, and Wineglass.</li>
            </ul>
            <img src="dataset_image.png" alt="Dataset Image">
        </section>
        
        <section id="features">
            <h2>Feature Extraction</h2>
            
            <div class="feature-box">
                <h3>1. Image Preprocessing & Local Descriptor Extraction</h3>
                <h4>1.1 Preprocessing</h4>
                <p><strong>Objective:</strong> Convert images into a uniform format suitable for feature extraction.</p>
                <p><strong>Process:</strong></p>
                <ul class="process-list">
                    <li>Convert each image to grayscale.</li>
                    <li>Resize all images to a fixed size (256×256) to standardize the descriptor grid.</li>
                </ul>
                
                <h4>1.2 Gradient Computation</h4>
                <p><strong>Objective:</strong> Extract edge information that is invariant to small changes in shape or appearance.</p>
                <p><strong>Process:</strong></p>
                <ul class="process-list">
                    <li>Apply Sobel filters to compute horizontal (grad_x) and vertical (grad_y) gradients.</li>
                    <li>Calculate the gradient magnitude and orientation for each pixel.</li>
                    <li>Map orientations to the range [0,π) for unsigned angles.</li>
                </ul>
                
                <h4>1.3 Local Descriptor Construction</h4>
                <p><strong>Objective:</strong> Describe local texture and structure using gradient histograms.</p>
                <p><strong>Process:</strong></p>
                <ul class="process-list">
                    <li>Divide image into patches using a fixed grid (e.g., 28×28 patches).</li>
                    <li>For each patch:
                        <ul>
                            <li>Subdivide it into spatial bins (e.g., 4×4 grid).</li>
                            <li>Compute a histogram of gradient orientations in each cell using magnitude as weight.</li>
                            <li>Concatenate all histograms to form a 64-dimensional descriptor per patch.</li>
                            <li>Normalize each descriptor to make it robust to illumination changes.</li>
                        </ul>
                    </li>
                </ul>
            </div>
            
            <div class="feature-box">
                <h3>2. Visual Vocabulary Creation Using K-Means</h3>
                <h4>2.1 Gathering Descriptors</h4>
                <p><strong>Objective:</strong> Collect all local descriptors from the dataset to build a visual vocabulary.</p>
                <p><strong>Process:</strong></p>
                <ul class="process-list">
                    <li>Iterate over all images in each class folder.</li>
                    <li>Extract local descriptors from each image using the previous step.</li>
                    <li>Store all descriptors together in a single array (for clustering).</li>
                    <li>Store image paths and their corresponding class labels.</li>
                </ul>
                
                <h4>2.2 Vocabulary Learning</h4>
                <p><strong>Objective:</strong> Create a visual vocabulary (codebook) by clustering local descriptors.</p>
                <p><strong>Process:</strong></p>
                <ul class="process-list">
                    <li>Apply K-Means clustering to all descriptors.</li>
                    <li>Each cluster center represents a "visual word".</li>
                    <li>The total number of clusters determines the vocabulary size.</li>
                    <li>Save the vocabulary (vocabulary.npy) for use during feature encoding.</li>
                </ul>
            </div>
            
            <div class="feature-box">
                <h3>3. Global Image Feature Extraction: Histogram of Visual Words</h3>
                <h4>3.1 Descriptor Quantization</h4>
                <p><strong>Objective:</strong> Convert each local descriptor into a visual word assignment.</p>
                <p><strong>Process:</strong></p>
                <ul class="process-list">
                    <li>For each descriptor, compute Euclidean distance to all cluster centers (vocabulary).</li>
                    <li>Assign it to the nearest cluster (visual word).</li>
                    <li>Build a histogram of visual word occurrences for the image.</li>
                    <li>Normalize the histogram to get a 500-dimensional global feature vector.</li>
                </ul>
                
                <h4>3.2 Dataset-Wide Feature Extraction</h4>
                <p><strong>Objective:</strong> Generate a BoVW histogram for every image in the dataset.</p>
                <p><strong>Process:</strong></p>
                <ul class="process-list">
                    <li>For each image, repeat the descriptor extraction and quantization process.</li>
                    <li>Store the global feature vector and its associated label.</li>
                </ul>
            </div>
            
            <div class="feature-box">
                <h3>4. Feature Storage and Visualization</h3>
                <h4>4.1 Saving Features</h4>
                <p><strong>Objective:</strong> Persist the computed features and labels for model training.</p>
                <p><strong>Process:</strong></p>
                <ul class="process-list">
                    <li>Save features as a NumPy array (image_features.npy).</li>
                    <li>Save corresponding labels (image_labels.npy).</li>
                </ul>
                
                <h4>4.2 Visualizing Feature Vectors</h4>
                <p><strong>Objective:</strong> Inspect feature vectors to understand representation quality.</p>
                <p><strong>Process:</strong></p>
                <ul class="process-list">
                    <li>For each class, find the first image in that class.</li>
                    <li>Extract and print its BoVW histogram.</li>
                    <li>This gives insight into how different or similar feature vectors are across categories.</li>
                </ul>
            </div>
            
            <div class="stats">
                <h3>Results</h3>
                <p>We obtained 940,800 descriptors from 1,200 images. The size of these images features is (1200,500). After clustering these descriptors, the Features and labels are stored in files - 'image_features.npy' and 'image_label.npy'</p>
            </div>
        </section>
        
        <section id="versions">
            <h2>Project Versions</h2>
            <p>We created two versions of this project to optimize our accuracy and build our functionalities accordingly.</p>
            
            <div class="version-card">
                <h3><span class="version-number">1</span> Version 1</h3>
                <ul class="process-list">
                    <li>It consists of 250 classes consisting of 20,000 images</li>
                    <li>The prediction of these algorithms is obtained through various techniques.</li>
                    <li>We implemented these models with multiple approaches.</li>
                </ul>
            </div>
            
            <div class="version-card">
                <h3><span class="version-number">2</span> Version 2</h3>
                <ul class="process-list">
                    <li>We reduced the number of classes to 15, consisting of 1,200 images.</li>
                    <li>The extraction of features is done through the above process.</li>
                    <li>Reducing the number of classes helps our model be more accurate. The model got confused by certain categories.</li>
                    <li>We enabled real-time prediction using Google Cloud storage. After we draw on the given application, press on "predict". Following, we get the model's prediction.</li>
                </ul>
            </div>
            <img src="version_2_image.png" alt="Version 2 Image">
        </section>
        
        <section id="methodologies">
            <h2>Methodology and Models</h2>
            <p>The project implements various machine learning models and techniques for sketch recognition. These methodologies are tailored for both versions of the project, with optimizations made based on the dataset size and classification complexity.</p>
            <p class="highlight">See the detailed methodology documentation for specific model implementations and performance metrics.</p>
        </section>
    </div>
    
    <!-- Scroll to top button -->
    <div class="scroll-top" id="scrollTop">
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <line x1="12" y1="19" x2="12" y2="5"></line>
            <polyline points="5 12 12 5 19 12"></polyline>
        </svg>
    </div>
    
    <footer>
        <div class="logo">Skribix</div>
        <p class="footer-text">Handdrawn Sketch Recognition Project</p>
    </footer>

    <script>
        // Navigation Toggle
        const navToggle = document.getElementById('navToggle');
        const sidebar = document.getElementById('sidebar');
        const overlay = document.getElementById('overlay');
        const scrollTop = document.getElementById('scrollTop');
        const navLinks = document.querySelectorAll('.sidebar ul li a');
        
        // Toggle sidebar function
        function toggleSidebar() {
            sidebar.classList.toggle('active');
            overlay.classList.toggle('active');
            navToggle.classList.toggle('active');
        }
        
        // Event listeners
        navToggle.addEventListener('click', toggleSidebar);
        overlay.addEventListener('click', toggleSidebar);
        
        // Close sidebar when a nav link is clicked
        navLinks.forEach(link => {
            link.addEventListener('click', () => {
                toggleSidebar();
            });
        });
        
        // Scroll to top button visibility
        window.addEventListener('scroll', () => {
            if (window.pageYOffset > 300) {
                scrollTop.classList.add('active');
            } else {
                scrollTop.classList.remove('active');
            }
        });
        
        // Scroll to top on button click
        scrollTop.addEventListener('click', () => {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });
        
        // Smooth scrolling for anchor links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function(e) {
                e.preventDefault();
                
                const targetId = this.getAttribute('href');
                const targetElement = document.querySelector(targetId);
                
                if (targetElement) {
                    window.scrollTo({
                        top: targetElement.offsetTop - 20,
                        behavior: 'smooth'
                    });
                }
            });
        });
    </script>
</body>
</html>