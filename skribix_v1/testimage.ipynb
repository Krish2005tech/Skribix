{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64bc4fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from tensorflow.keras.models import load_model\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd52a4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, size=256):\n",
    "    \"\"\"\n",
    "    Load the image in grayscale, resize it to size x size (256x256),\n",
    "    and center it if needed.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise ValueError(\"Image not found or unable to load.\")\n",
    "    # Resize image to 256x256\n",
    "    img_resized = cv2.resize(img, (size, size))\n",
    "    return img_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "339e01e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(image):\n",
    "    \"\"\"\n",
    "    Compute image gradients using Sobel operator.\n",
    "    (In the paper, Gaussian derivatives are used for more robust estimates.)\n",
    "    \"\"\"\n",
    "    grad_x = cv2.Sobel(image, cv2.CV_32F, 1, 0, ksize=3)\n",
    "    grad_y = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize=3)\n",
    "    magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "    orientation = np.arctan2(grad_y, grad_x)  # Angle in radians\n",
    "    # Map orientations to [0, π)\n",
    "    orientation = np.mod(orientation, np.pi)\n",
    "    return magnitude, orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a782f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_local_features(image, grid_size=(28, 28), patch_ratio=0.125, num_bins=4):\n",
    "    \"\"\"\n",
    "    Extract local descriptors from the image on a regular grid.\n",
    "    Each descriptor is a histogram of gradient orientations computed\n",
    "    from a patch (patch size = 12.5% of image size).\n",
    "    \"\"\"\n",
    "    magnitude, orientation = compute_gradients(image)\n",
    "    features = []\n",
    "    h, w = image.shape\n",
    "    # Determine grid points uniformly across the image\n",
    "    xs = np.linspace(0, w-1, grid_size[1], dtype=int)\n",
    "    ys = np.linspace(0, h-1, grid_size[0], dtype=int)\n",
    "    patch_size = int(patch_ratio * w)  # e.g., 0.125*256 ≈ 32 pixels\n",
    "    half_patch = patch_size // 2\n",
    "\n",
    "    for y in ys:\n",
    "        for x in xs:\n",
    "            # Define patch boundaries with proper handling at the borders\n",
    "            x1 = max(x - half_patch, 0)\n",
    "            x2 = min(x + half_patch, w)\n",
    "            y1 = max(y - half_patch, 0)\n",
    "            y2 = min(y + half_patch, h)\n",
    "            # Extract patch gradients and orientations\n",
    "            patch_orient = orientation[y1:y2, x1:x2]\n",
    "            patch_mag = magnitude[y1:y2, x1:x2]\n",
    "            # Compute weighted histogram of orientations in the patch\n",
    "            hist, _ = np.histogram(patch_orient, bins=num_bins, range=(0, np.pi), weights=patch_mag)\n",
    "            # Normalize the histogram\n",
    "            norm = np.linalg.norm(hist)\n",
    "            if norm > 0:\n",
    "                hist = hist / norm\n",
    "            features.append(hist)\n",
    "    features = np.array(features)  # Shape: (number_of_patches, num_bins)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce452590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_features(features, vocabulary):\n",
    "    \"\"\"\n",
    "    Quantize each feature by assigning it to the closest visual word.\n",
    "    Uses hard assignment (nearest neighbor) based on Euclidean distance.\n",
    "    \"\"\"\n",
    "    # vocabulary is assumed to be of shape (num_words, feature_dim)\n",
    "    distances = cdist(features, vocabulary, 'euclidean')\n",
    "    word_indices = np.argmin(distances, axis=1)\n",
    "    return word_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "887fd69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_histogram(word_indices, num_words):\n",
    "    \"\"\"\n",
    "    Build a normalized histogram of visual words.\n",
    "    \"\"\"\n",
    "    hist = np.zeros(num_words)\n",
    "    for idx in word_indices:\n",
    "        hist[idx] += 1\n",
    "    # Normalize the histogram by the total number of features\n",
    "    if np.sum(hist) > 0:\n",
    "        hist = hist / np.sum(hist)\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16b8b9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(hist, svm_model):\n",
    "    \"\"\"\n",
    "    Predict the category using the pre-trained multi-class SVM.\n",
    "    Here, svm_model is assumed to be a scikit-learn compatible classifier.\n",
    "    \"\"\"\n",
    "    # Reshape histogram to (1, -1) as the model expects a 2D array\n",
    "    pred = svm_model.predict(hist.reshape(1, -1))\n",
    "    return pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7263865",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The MAT file does not contain 'vocabulary'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m mat_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_extraction/features_shog_smooth.mat\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     35\u001b[0m model_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_ann_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 36\u001b[0m main(test_image_path, mat_file_path, model_file_path)\n",
      "Cell \u001b[0;32mIn[17], line 12\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(test_image_path, mat_file_path, model_file_path)\u001b[0m\n\u001b[1;32m     10\u001b[0m data \u001b[38;5;241m=\u001b[39m loadmat(mat_file_path)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvocabulary\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe MAT file does not contain \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvocabulary\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m vocabulary \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvocabulary\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Expected shape: (num_words, feature_dim)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m num_words \u001b[38;5;241m=\u001b[39m vocabulary\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: The MAT file does not contain 'vocabulary'."
     ]
    }
   ],
   "source": [
    "def main(test_image_path, mat_file_path, model_file_path):\n",
    "    # Step 1: Load and preprocess the test image\n",
    "    img = preprocess_image(test_image_path)\n",
    "    \n",
    "    # Step 2: Extract local features from the test image\n",
    "    features = extract_local_features(img)\n",
    "    \n",
    "    # Step 3: Load the visual vocabulary from the .mat file.\n",
    "    # It is assumed that the MAT file contains a variable 'vocabulary'\n",
    "    data = loadmat(mat_file_path)\n",
    "    if 'vocabulary' not in data:\n",
    "        raise ValueError(\"The MAT file does not contain 'vocabulary'.\")\n",
    "    vocabulary = data['vocabulary']  # Expected shape: (num_words, feature_dim)\n",
    "    num_words = vocabulary.shape[0]\n",
    "    \n",
    "    # Step 4: Quantize the features using the visual vocabulary\n",
    "    word_indices = quantize_features(features, vocabulary)\n",
    "    \n",
    "    # Step 5: Build the normalized histogram of visual words (feature representation)\n",
    "    hist = build_histogram(word_indices, num_words)\n",
    "    \n",
    "    # Step 6: Load the pre-trained SVM model (assumed to be saved as a pickle file)\n",
    "    with open(model_file_path, 'rb') as f:\n",
    "        svm_model = load_model(\"best_ann_model.h5\")\n",
    "    \n",
    "    # Step 7: Predict the category of the test image using the SVM classifier\n",
    "    prediction = svm_model.predict(hist.reshape(1, -1))\n",
    "    predicted_class = prediction.argmax(axis=1)[0]\n",
    "    print(\"Predicted class:\", predicted_class)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Replace these paths with the actual paths on your system.\n",
    "    test_image_path = 'path_to_your_test_image.png'\n",
    "    mat_file_path = 'feature_shog_smooth.mat'\n",
    "    model_file_path = 'best_ann_model.h5'\n",
    "    main(test_image_path, mat_file_path, model_file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c66ac38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluating precomputed dataset (1-NN) ===\n",
      "1-NN classification accuracy on test partition: 24.90% (chance ~0.4%)\n",
      "\n",
      "=== Testing new image with ANN model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "Predicted category for the test image: 73\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from tensorflow.keras.models import load_model\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def evaluate_dataset(mat_file_path):\n",
    "    \"\"\"\n",
    "    Load the MAT file containing the precomputed features.\n",
    "    The MAT file is assumed to have a matrix 'A' with size 20,000x502:\n",
    "      - Column 0: category id in [1,250]\n",
    "      - Column 1: partition id in [1,10]\n",
    "      - Columns 2-501: the 500-dimensional feature vector.\n",
    "      \n",
    "    This function splits the data into training (partition==1) and test \n",
    "    (partition==2) sets, then performs 1-NN classification and prints accuracy.\n",
    "    \"\"\"\n",
    "    # Load MAT file\n",
    "    data = loadmat(mat_file_path)\n",
    "    if 'A' not in data:\n",
    "        raise ValueError(\"MAT file does not contain variable 'A'\")\n",
    "    A = data['A']\n",
    "    \n",
    "    # Split into training and testing partitions based on column 1 (partition id)\n",
    "    partition_train = (A[:, 1] == 1)\n",
    "    partition_test  = (A[:, 1] == 2)\n",
    "    \n",
    "    # Training features: columns 2 to end; ground truth: column 0\n",
    "    M = A[partition_train, 2:]\n",
    "    categories_train = A[partition_train, 0]\n",
    "    \n",
    "    # Test features and labels\n",
    "    N = A[partition_test, 2:]\n",
    "    categories_test = A[partition_test, 0]\n",
    "    \n",
    "    # Compute pairwise squared Euclidean distances between training and test features.\n",
    "    # D[i,j] is the distance between training sample i and test sample j.\n",
    "    D = (np.sum(M**2, axis=1).reshape(-1, 1) \n",
    "         - 2 * M.dot(N.T) \n",
    "         + np.sum(N**2, axis=1).reshape(1, -1))\n",
    "    \n",
    "    # For each test sample, find the index of the closest training sample.\n",
    "    nearest_idx = np.argmin(D, axis=0)\n",
    "    # Predicted categories are the categories of the nearest neighbors.\n",
    "    categories_predicted = categories_train[nearest_idx]\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(categories_predicted == categories_test) * 100\n",
    "    print(f\"1-NN classification accuracy on test partition: {accuracy:.2f}% (chance ~0.4%)\")\n",
    "\n",
    "def preprocess_image(image_path, size=256):\n",
    "    \"\"\"\n",
    "    Load a sketch image in grayscale and resize it to a square of given size.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise ValueError(\"Image not found or unable to load.\")\n",
    "    img_resized = cv2.resize(img, (size, size))\n",
    "    return img_resized\n",
    "\n",
    "def extract_feature_from_image(image):\n",
    "    \"\"\"\n",
    "    Extract a 500-dimensional feature vector from the input sketch image.\n",
    "    \n",
    "    This is a placeholder function. In the paper the feature extraction involves:\n",
    "      - Rescaling the image to 256x256,\n",
    "      - Computing image gradients (e.g. with Gaussian derivatives),\n",
    "      - Sampling on a 28x28 grid,\n",
    "      - Constructing spatial histograms (using soft kernel codebook coding)\n",
    "      - Aggregating the descriptors into a 500-dim feature vector.\n",
    "    \n",
    "    Replace this placeholder with your actual feature extraction code.\n",
    "    \"\"\"\n",
    "    # --- Placeholder implementation ---\n",
    "    # Here, we simply flatten a resized image and then project (dummy operation)\n",
    "    # to obtain a 500-dimensional vector.\n",
    "    img_flat = image.flatten().astype(np.float32)\n",
    "    # Create a dummy projection matrix to simulate feature extraction.\n",
    "    # In practice, you would replace this with your own extraction pipeline.\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    projection = np.random.rand(img_flat.shape[0], 500).astype(np.float32)\n",
    "    feature_vector = img_flat.dot(projection)\n",
    "    # Normalize the feature vector\n",
    "    norm = np.linalg.norm(feature_vector)\n",
    "    if norm > 0:\n",
    "        feature_vector /= norm\n",
    "    # --- End placeholder ---\n",
    "    return feature_vector\n",
    "\n",
    "def predict_category(feature_vector, model_file_path):\n",
    "    \"\"\"\n",
    "    Load the pre-trained ANN model from an H5 file and predict the category\n",
    "    given a 500-dimensional feature vector.\n",
    "    \"\"\"\n",
    "    # Load the Keras ANN model (do not use pickle for H5 files)\n",
    "    ann_model = load_model(model_file_path)\n",
    "    \n",
    "    # Reshape the feature vector for the model (batch_size, feature_dim)\n",
    "    feature_vector = feature_vector.reshape(1, -1)\n",
    "    \n",
    "    # Predict using the ANN model\n",
    "    predictions = ann_model.predict(feature_vector)\n",
    "    \n",
    "    # The predicted category is the index with the highest score.\n",
    "    # If your category labels are 1-indexed (1 to 250), add 1.\n",
    "    predicted_class = predictions.argmax(axis=1)[0] + 1\n",
    "    return predicted_class\n",
    "\n",
    "def test_new_image(test_image_path, model_file_path):\n",
    "    \"\"\"\n",
    "    Process a user-made sketch image, extract its feature vector,\n",
    "    and use the ANN model to predict its category.\n",
    "    \"\"\"\n",
    "    # Step 1: Preprocess the image\n",
    "    img = preprocess_image(test_image_path)\n",
    "    \n",
    "    # Step 2: Extract a 500-dimensional feature vector\n",
    "    feature_vector = extract_feature_from_image(img)\n",
    "    \n",
    "    # Step 3: Predict the category using the ANN model\n",
    "    category = predict_category(feature_vector, model_file_path)\n",
    "    print(\"Predicted category for the test image:\", category)\n",
    "\n",
    "def main():\n",
    "    # File paths (update these paths according to your setup)\n",
    "    mat_file_path = 'feature_extraction/features_shog_smooth.mat'\n",
    "    model_file_path = 'best_ann_model.h5'\n",
    "    test_image_path = 'sketches/windmill/19602.png'\n",
    "    \n",
    "    print(\"=== Evaluating precomputed dataset (1-NN) ===\")\n",
    "    evaluate_dataset(mat_file_path)\n",
    "    \n",
    "    print(\"\\n=== Testing new image with ANN model ===\")\n",
    "    test_new_image(test_image_path, model_file_path)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efd185d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluating precomputed dataset (1-NN) ===\n",
      "1-NN classification accuracy on test partition: 24.90% (chance ~0.4%)\n",
      "\n",
      "=== Testing new image with ANN model ===\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Vocabulary file 'vocabulary.npy' not found. Ensure you have a (500,64) vocabulary.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 164\u001b[0m, in \u001b[0;36mtest_new_image\u001b[0;34m(test_image_path, model_file_path, vocabulary_path)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(vocabulary_path)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mopen\u001b[39m(os_fspath(file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'vocabulary.npy'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 186\u001b[0m\n\u001b[1;32m    183\u001b[0m     test_new_image(test_image_path, model_file_path, vocabulary_path)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 186\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[30], line 183\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    180\u001b[0m evaluate_dataset(mat_file_path)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Testing new image with ANN model ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 183\u001b[0m test_new_image(test_image_path, model_file_path, vocabulary_path)\n",
      "Cell \u001b[0;32mIn[30], line 166\u001b[0m, in \u001b[0;36mtest_new_image\u001b[0;34m(test_image_path, model_file_path, vocabulary_path)\u001b[0m\n\u001b[1;32m    164\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(vocabulary_path)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocabulary file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvocabulary_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found. Ensure you have a (500,64) vocabulary.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    168\u001b[0m feature_vector \u001b[38;5;241m=\u001b[39m extract_feature(img, vocabulary)\n\u001b[1;32m    169\u001b[0m category \u001b[38;5;241m=\u001b[39m predict_category(feature_vector, model_file_path)\n",
      "\u001b[0;31mValueError\u001b[0m: Vocabulary file 'vocabulary.npy' not found. Ensure you have a (500,64) vocabulary."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from tensorflow.keras.models import load_model\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# ----------------------------\n",
    "# Feature extraction functions\n",
    "# ----------------------------\n",
    "\n",
    "def preprocess_image(image_path, size=256):\n",
    "    \"\"\"\n",
    "    Load a sketch image in grayscale, resize to size x size.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Image not found or unable to load: {image_path}\")\n",
    "    img_resized = cv2.resize(img, (size, size))\n",
    "    return img_resized.astype(np.float32)\n",
    "\n",
    "def compute_gradients(image):\n",
    "    \"\"\"\n",
    "    Compute image gradients using Sobel operator.\n",
    "    (Note: In the paper, Gaussian derivatives are used for robustness.)\n",
    "    \"\"\"\n",
    "    grad_x = cv2.Sobel(image, cv2.CV_32F, 1, 0, ksize=3)\n",
    "    grad_y = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize=3)\n",
    "    magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "    orientation = np.arctan2(grad_y, grad_x)\n",
    "    orientation = np.mod(orientation, np.pi)  # Map to [0, π)\n",
    "    return magnitude, orientation\n",
    "\n",
    "def extract_local_descriptors(image, grid_size=(28, 28), patch_size_ratio=0.125,\n",
    "                              num_spatial_bins=4, num_orientation_bins=4):\n",
    "    \"\"\"\n",
    "    Extract a 64-dimensional descriptor for each patch.\n",
    "    Each patch (of size patch_size_ratio * image width) is subdivided into\n",
    "    num_spatial_bins x num_spatial_bins cells. For each cell, a histogram\n",
    "    of gradient orientations (with num_orientation_bins bins) is computed.\n",
    "    The descriptors from all cells are concatenated (resulting in a 64-D vector).\n",
    "    \"\"\"\n",
    "    magnitude, orientation = compute_gradients(image)\n",
    "    h, w = image.shape\n",
    "    descriptors = []\n",
    "    xs = np.linspace(0, w-1, grid_size[1], dtype=int)\n",
    "    ys = np.linspace(0, h-1, grid_size[0], dtype=int)\n",
    "    patch_size = int(patch_size_ratio * w)  # e.g., 0.125 * 256 ≈ 32 pixels\n",
    "    half_patch = patch_size // 2\n",
    "\n",
    "    for y in ys:\n",
    "        for x in xs:\n",
    "            # Define patch boundaries\n",
    "            x1 = max(x - half_patch, 0)\n",
    "            x2 = min(x + half_patch, w)\n",
    "            y1 = max(y - half_patch, 0)\n",
    "            y2 = min(y + half_patch, h)\n",
    "            patch_mag = magnitude[y1:y2, x1:x2]\n",
    "            patch_orient = orientation[y1:y2, x1:x2]\n",
    "            # Determine cell size\n",
    "            cell_h = (y2 - y1) // num_spatial_bins\n",
    "            cell_w = (x2 - x1) // num_spatial_bins\n",
    "            descriptor = []\n",
    "            # For each cell in the patch\n",
    "            for i in range(num_spatial_bins):\n",
    "                for j in range(num_spatial_bins):\n",
    "                    cy1 = y1 + i * cell_h\n",
    "                    cy2 = cy1 + cell_h\n",
    "                    cx1 = x1 + j * cell_w\n",
    "                    cx2 = cx1 + cell_w\n",
    "                    # Ensure indices are within patch bounds\n",
    "                    cell_orient = patch_orient[cy1 - y1:cy2 - y1, cx1 - x1:cx2 - x1]\n",
    "                    cell_mag = patch_mag[cy1 - y1:cy2 - y1, cx1 - x1:cx2 - x1]\n",
    "                    # Compute histogram for the cell\n",
    "                    hist, _ = np.histogram(cell_orient, bins=num_orientation_bins, range=(0, np.pi), weights=cell_mag)\n",
    "                    descriptor.extend(hist)\n",
    "            descriptor = np.array(descriptor, dtype=np.float32)\n",
    "            # Normalize the descriptor\n",
    "            norm = np.linalg.norm(descriptor)\n",
    "            if norm > 0:\n",
    "                descriptor /= norm\n",
    "            descriptors.append(descriptor)\n",
    "    return np.array(descriptors)  # Shape: (num_patches, 64)\n",
    "\n",
    "def quantize_descriptors(descriptors, vocabulary):\n",
    "    \"\"\"\n",
    "    Quantize each 64-D descriptor to the nearest word in the vocabulary.\n",
    "    vocabulary is expected to be a (500, 64) NumPy array.\n",
    "    Returns a normalized histogram (500-D vector).\n",
    "    \"\"\"\n",
    "    # Compute Euclidean distances between each descriptor and each vocabulary word\n",
    "    distances = cdist(descriptors, vocabulary, metric='euclidean')\n",
    "    assignments = np.argmin(distances, axis=1)  # Hard assignment\n",
    "    hist = np.zeros(vocabulary.shape[0], dtype=np.float32)\n",
    "    for idx in assignments:\n",
    "        hist[idx] += 1\n",
    "    # Normalize the histogram\n",
    "    if hist.sum() > 0:\n",
    "        hist /= hist.sum()\n",
    "    return hist\n",
    "\n",
    "def extract_feature(image, vocabulary):\n",
    "    \"\"\"\n",
    "    Extract the 500-D feature for the image by computing local descriptors\n",
    "    and quantizing them using the provided vocabulary.\n",
    "    \"\"\"\n",
    "    descriptors = extract_local_descriptors(image)\n",
    "    feature_vector = quantize_descriptors(descriptors, vocabulary)\n",
    "    return feature_vector  # 500-dimensional vector\n",
    "\n",
    "# ----------------------------\n",
    "# Prediction and evaluation\n",
    "# ----------------------------\n",
    "\n",
    "def evaluate_dataset(mat_file_path):\n",
    "    \"\"\"\n",
    "    Evaluate 1-NN on precomputed features stored in the MAT file.\n",
    "    (This function uses the features provided in the MAT file.)\n",
    "    \"\"\"\n",
    "    data = loadmat(mat_file_path)\n",
    "    if 'A' not in data:\n",
    "        raise ValueError(\"MAT file does not contain variable 'A'\")\n",
    "    A = data['A']\n",
    "    \n",
    "    partition_train = (A[:, 1] == 1)\n",
    "    partition_test  = (A[:, 1] == 2)\n",
    "    \n",
    "    M = A[partition_train, 2:]\n",
    "    categories_train = A[partition_train, 0]\n",
    "    \n",
    "    N = A[partition_test, 2:]\n",
    "    categories_test = A[partition_test, 0]\n",
    "    \n",
    "    D = (np.sum(M**2, axis=1).reshape(-1, 1) \n",
    "         - 2 * M.dot(N.T) \n",
    "         + np.sum(N**2, axis=1).reshape(1, -1))\n",
    "    \n",
    "    nearest_idx = np.argmin(D, axis=0)\n",
    "    categories_predicted = categories_train[nearest_idx]\n",
    "    \n",
    "    accuracy = np.mean(categories_predicted == categories_test) * 100\n",
    "    print(f\"1-NN classification accuracy on test partition: {accuracy:.2f}% (chance ~0.4%)\")\n",
    "\n",
    "def predict_category(feature_vector, model_file_path):\n",
    "    \"\"\"\n",
    "    Load the pre-trained ANN model from an H5 file and predict the category\n",
    "    for the given 500-D feature vector.\n",
    "    \"\"\"\n",
    "    # Ensure feature_vector is reshaped to (1, 500)\n",
    "    feature_vector = feature_vector.reshape(1, -1)\n",
    "    ann_model = load_model(model_file_path)\n",
    "    predictions = ann_model.predict(feature_vector)\n",
    "    predicted_class = predictions.argmax(axis=1)[0] + 1  # Adjust if categories are 1-indexed.\n",
    "    return predicted_class\n",
    "\n",
    "def test_new_image(test_image_path, model_file_path, vocabulary_path='vocabulary.npy'):\n",
    "    \"\"\"\n",
    "    Process a user-made sketch image, extract its 500-D feature vector using\n",
    "    the visual vocabulary, and use the ANN model to predict its category.\n",
    "    \"\"\"\n",
    "    img = preprocess_image(test_image_path)\n",
    "    \n",
    "    # Load the visual vocabulary (should be a NumPy file of shape (500, 64))\n",
    "    try:\n",
    "        vocabulary = np.load(vocabulary_path)\n",
    "    except FileNotFoundError:\n",
    "        raise ValueError(f\"Vocabulary file '{vocabulary_path}' not found. Ensure you have a (500,64) vocabulary.\")\n",
    "    \n",
    "    feature_vector = extract_feature(img, vocabulary)\n",
    "    category = predict_category(feature_vector, model_file_path)\n",
    "    print(\"Predicted category for the test image:\", category)\n",
    "\n",
    "def main():\n",
    "    # Update these paths according to your setup\n",
    "    mat_file_path = 'feature_extraction/features_shog_smooth.mat'\n",
    "    model_file_path = 'best_ann_model.h5'\n",
    "    test_image_path = 'sketches/pen/11921.png'\n",
    "    vocabulary_path = 'vocabulary.npy'  # This file must exist (shape: 500x64)\n",
    "    \n",
    "    print(\"=== Evaluating precomputed dataset (1-NN) ===\")\n",
    "    evaluate_dataset(mat_file_path)\n",
    "    \n",
    "    print(\"\\n=== Testing new image with ANN model ===\")\n",
    "    test_new_image(test_image_path, model_file_path, vocabulary_path)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73e5464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
