{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, output_size=(256, 256)):\n",
    "    # Load the image in grayscale mode\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        print(\"Error loading image:\", image_path)\n",
    "        return None\n",
    "\n",
    "    # Binarize the image using Otsu's thresholding\n",
    "    # This converts the image to a binary image (0 and 255)\n",
    "    _, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Optionally, you can perform additional morphological operations here\n",
    "    # For example, if you want to remove noise or fill small gaps:\n",
    "    # kernel = np.ones((3, 3), np.uint8)\n",
    "    # binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    # Normalize the image size by resizing\n",
    "    binary_resized = cv2.resize(binary, output_size, interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    return binary_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(input_folder, output_folder, output_size=(256, 256)):\n",
    "    # Create the output directory if it does not exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Get list of image files in the input folder (assuming PNG images)\n",
    "    image_files = glob(os.path.join(input_folder, \"**\", \"*.png\"), recursive=True)\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images in {input_folder}.\")\n",
    "    \n",
    "    # Process each image\n",
    "    for image_path in image_files:\n",
    "        preprocessed = preprocess_image(image_path, output_size)\n",
    "        if preprocessed is not None:\n",
    "            filename = os.path.basename(image_path)\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "            cv2.imwrite(output_path, preprocessed)\n",
    "    \n",
    "    print(\"Preprocessing complete. Preprocessed images saved to:\", output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images in ./sketches.\n",
      "Preprocessing complete. Preprocessed images saved to: ./processed_sketches\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set the paths to your dataset directory and the directory to save processed images.\n",
    "    input_folder = \"./sketches\"       \n",
    "    output_folder = \"./processed_sketches\"  \n",
    "    \n",
    "    output_size = (256, 256)\n",
    "    \n",
    "    process_dataset(input_folder, output_folder, output_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation of pixels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images normalized and saved in 'normalized_sketches'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# defining path for input and output folders!\n",
    "FOLDER_PATH = \"processed_sketches\"\n",
    "NORMALIZED_PATH = \"normalized_sketches\" \n",
    "\n",
    "# ensures output folder exists\n",
    "os.makedirs(NORMALIZED_PATH, exist_ok=True)\n",
    "\n",
    "def normalize_images(folder_path, output_folder):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Read image in grayscale\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            print(f\"Skipping {filename} (not a valid image)\")\n",
    "            continue\n",
    "\n",
    "        # Normalize pixel values to range [0,1]\n",
    "        img_normalized = img.astype(\"float32\") / 255.0\n",
    "        # Convert back to 8-bit for saving as PNG/JPG\n",
    "        img_uint8 = (img_normalized * 255).astype(\"uint8\")\n",
    "        # Save the normalized image\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        cv2.imwrite(output_path, img_uint8)\n",
    "\n",
    "    print(f\"All images normalized and saved in '{output_folder}'\")\n",
    "\n",
    "# Run fucntion\n",
    "normalize_images(FOLDER_PATH, NORMALIZED_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from skimage import io, color\n",
    "from skimage.feature import hog\n",
    "from skimage.filters import gabor\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_features(image, pixels_per_cell=(16, 16), cells_per_block=(2, 2), orientations=9):\n",
    "    \"\"\"\n",
    "    Extract HOG features from a grayscale image.\n",
    "    \"\"\"\n",
    "    # skimage.feature.hog returns a flat vector when feature_vector=True.\n",
    "    hog_features = hog(image,\n",
    "                       orientations=orientations,\n",
    "                       pixels_per_cell=pixels_per_cell,\n",
    "                       cells_per_block=cells_per_block,\n",
    "                       block_norm='L2-Hys',\n",
    "                       transform_sqrt=True,\n",
    "                       feature_vector=True)\n",
    "    return hog_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gabor_features(image, frequencies=[0.1, 0.2, 0.3], orientations=[0, np.pi/4, np.pi/2, 3*np.pi/4]):\n",
    "    \"\"\"\n",
    "    Extract Gabor filter responses from the image.\n",
    "    For each frequency and orientation, we compute the mean and variance\n",
    "    of the magnitude response, then concatenate these values.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for freq in frequencies:\n",
    "        for theta in orientations:\n",
    "            filt_real, filt_imag = gabor(image, frequency=freq, theta=theta)\n",
    "            magnitude = np.sqrt(filt_real**2 + filt_imag**2)\n",
    "            features.append(magnitude.mean())\n",
    "            features.append(magnitude.var())\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hybrid_features(image):\n",
    "    \"\"\"\n",
    "    Extract hybrid features by combining HOG and Gabor-based features.\n",
    "    Assumes image is a normalized grayscale image.\n",
    "    \"\"\"\n",
    "    hog_feat = extract_hog_features(image)\n",
    "    gabor_feat = extract_gabor_features(image)\n",
    "    # Concatenate the two feature vectors\n",
    "    hybrid_feat = np.concatenate([hog_feat, gabor_feat])\n",
    "    return hybrid_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_matrix(images):\n",
    "    \"\"\"\n",
    "    Given a list of preprocessed images, extract hybrid features for each.\n",
    "    Returns a 2D array where each row is a hybrid feature vector.\n",
    "    \"\"\"\n",
    "    feature_list = []\n",
    "    for idx, image in enumerate(images):\n",
    "        feat = extract_hybrid_features(image)\n",
    "        feature_list.append(feat)\n",
    "        if (idx + 1) % 50 == 0:\n",
    "            print(f\"Extracted features from {idx+1} images.\")\n",
    "    return np.vstack(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_mean_shift_clustering(X_features, bandwidth=None):\n",
    "    \"\"\"\n",
    "    Apply Mean Shift clustering on the feature matrix.\n",
    "    Returns the cluster labels and the fitted MeanShift model.\n",
    "    \"\"\"\n",
    "    ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    ms.fit(X_features)\n",
    "    labels = ms.labels_\n",
    "    print(f\"Mean Shift found {len(np.unique(labels))} clusters.\")\n",
    "    return labels, ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(X_features, y_labels):\n",
    "    \"\"\"\n",
    "    Split the data into training and testing sets, then train a multi-class SVM\n",
    "    classifier and evaluate its performance.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_features, y_labels, test_size=0.3, random_state=42, stratify=y_labels)\n",
    "    \n",
    "    # Use a linear SVM; you may experiment with other kernels as well.\n",
    "    clf = SVC(kernel='linear', probability=True, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Test Accuracy: {acc*100:.2f}%\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set the path to your dataset (folder structure: dataset/<class_name>/*.png)\n",
    "    NORMALIZED_PATH = \"normalized_sketches\"\n",
    "\n",
    "    print(\"Loading dataset...\")\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(NORMALIZED_PATH):\n",
    "        img_path = os.path.join(NORMALIZED_PATH, filename)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "            labels.append(0)  \n",
    "            \n",
    "    print(f\"Loaded {len(images)} images from {len(set(labels))} classes.\")\n",
    "    \n",
    "    print(\"Extracting hybrid features (HOG + Gabor) from images...\")\n",
    "    X_features = build_feature_matrix(images)\n",
    "    print(\"Feature matrix shape:\", X_features.shape)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Unsupervised Clustering using Mean Shift\n",
    "    # ----------------------------\n",
    "    print(\"Performing Mean Shift clustering on the features...\")\n",
    "    ms_labels, ms_model = perform_mean_shift_clustering(X_features)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Train and Evaluate a Classifier\n",
    "    # ----------------------------\n",
    "    print(\"Training and evaluating a multi-class SVM classifier...\")\n",
    "    classifier = train_and_evaluate(X_features, np.array(labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Mean Shift Clustering Algorithm**\n",
    "\n",
    "Mean Shift is a **centroid-based** clustering algorithm. It iteratively shifts data points towards denser regions (clusters move towards higher density), making it useful for identifying clusters **without predefining the number of clusters**, which suits best for image classification. It is based on **Kernel Density Estimation (KDE)** and moves points towards higher-density areas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MeanShiftImageSegmentation:\n",
    "    def __init__(self, bandwidth=30, spatial_weight=0.1, tol=1):\n",
    "        \"\"\"\n",
    "        Initialize Mean Shift for image segmentation.\n",
    "\n",
    "        Parameters:\n",
    "        - bandwidth: Controls how much a pixel can shift.\n",
    "        - spatial_weight: Weighs spatial coordinates (to avoid grouping far pixels).\n",
    "        - tol: Convergence threshold.\n",
    "        \"\"\"\n",
    "        self.bandwidth = bandwidth\n",
    "        self.spatial_weight = spatial_weight\n",
    "        self.tol = tol\n",
    "\n",
    "    def fit(self, image):\n",
    "        \"\"\"\n",
    "        Applies Mean Shift to segment the given image.\n",
    "\n",
    "        Parameters:\n",
    "        - image: Input image (H, W, 3) in RGB format.\n",
    "\n",
    "        Returns:\n",
    "        - segmented_image: Image after Mean Shift segmentation.\n",
    "        \"\"\"\n",
    "        # Convert image to float and reshape to feature space\n",
    "        h, w, c = image.shape\n",
    "        img_lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)  # Convert to LAB color space\n",
    "        pixels = np.reshape(img_lab, (h * w, c)).astype(np.float32)\n",
    "\n",
    "        # Add spatial (x, y) coordinates\n",
    "        X, Y = np.meshgrid(np.arange(w), np.arange(h))\n",
    "        spatial_features = np.stack((X.ravel(), Y.ravel()), axis=1).astype(np.float32)\n",
    "        features = np.concatenate((pixels, self.spatial_weight * spatial_features), axis=1)\n",
    "\n",
    "        # Mean Shift clustering\n",
    "        for _ in range(10):  # Limit to 10 iterations for efficiency\n",
    "            new_features = np.copy(features)\n",
    "            for i in range(features.shape[0]):\n",
    "                distances = np.linalg.norm(features - features[i], axis=1)\n",
    "                weights = np.exp(-0.5 * (distances / self.bandwidth) ** 2)\n",
    "                new_features[i] = np.sum(features * weights[:, None], axis=0) / np.sum(weights)\n",
    "\n",
    "            # Check for convergence\n",
    "            shift_distance = np.linalg.norm(new_features - features, axis=1)\n",
    "            if np.max(shift_distance) < self.tol:\n",
    "                break\n",
    "            features = new_features\n",
    "\n",
    "        # Assign colors based on clusters\n",
    "        clustered_pixels = new_features[:, :3].reshape(h, w, c).astype(np.uint8)\n",
    "        segmented_image = cv2.cvtColor(clustered_pixels, cv2.COLOR_LAB2RGB)  # Convert back to RGB\n",
    "\n",
    "        return segmented_image\n",
    "\n",
    "# If we want to run it on a folder-> add here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
